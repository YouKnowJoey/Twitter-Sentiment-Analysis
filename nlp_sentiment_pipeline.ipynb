{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9804d85",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Pipeline Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20ed928",
   "metadata": {},
   "source": [
    "This project explores Natural Language Processing (NLP) techniques to analyze and classify the emotional tone and sentiment, expressed in social media text data in tweets from X (formerly Twitter). Using a combination of real-time data retrieval and supervised learning, we aim to build models that can accurately predict sentiment as positive or negative.\n",
    "\n",
    "We use the Sentiment140 dataset, accessed via Hugging Face Datasets, as our training and evaluation foundation. This dataset provides a large corpus of pre-labeled tweets, making it ideal for benchmarking model performance. Real-time tweets are fetched using the Tweepy library and authenticated access to the Twitter API.\n",
    "\n",
    "The main objective is to evaluate multiple machine learning classifiers such as Logistic Regression, Random Forest, and others-to determine which model performs best on the sentiment classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62e9b6e",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505f36c7",
   "metadata": {},
   "source": [
    "First, we will import the nessecary libraries used for the entire project. We will evaluate and reduce the dimensions of the imported data (via tensorflow_datasets).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22999aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "from sklearn.ensemble import GradientBoostingClassifier as GBM\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "import datasets\n",
    "import twitter_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793f999c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 81.4M/81.4M [00:02<00:00, 32.1MB/s]\n",
      "Generating train split: 100%|██████████| 1600000/1600000 [00:59<00:00, 26752.54 examples/s]\n",
      "Generating test split: 100%|██████████| 498/498 [00:00<00:00, 16597.38 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\", 'date': 'Mon Apr 06 22:19:45 PDT 2009', 'user': '_TheSpecialOne_', 'sentiment': 0, 'query': 'NO_QUERY'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### Load Training Data for models ###\n",
    "\n",
    "# Load Sentiment140 directly from Hugging Face datasets\n",
    "dataset = datasets.load_dataset(\"sentiment140\", trust_remote_code=True)\n",
    "\n",
    "train_data = dataset[\"train\"]\n",
    "\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921e7d1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>user</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>0</td>\n",
       "      <td>NO_QUERY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>0</td>\n",
       "      <td>NO_QUERY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>0</td>\n",
       "      <td>NO_QUERY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>0</td>\n",
       "      <td>NO_QUERY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>0</td>\n",
       "      <td>NO_QUERY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1  is upset that he can't update his Facebook by ...   \n",
       "2  @Kenichan I dived many times for the ball. Man...   \n",
       "3    my whole body feels itchy and like its on fire    \n",
       "4  @nationwideclass no, it's not behaving at all....   \n",
       "\n",
       "                           date             user  sentiment     query  \n",
       "0  Mon Apr 06 22:19:45 PDT 2009  _TheSpecialOne_          0  NO_QUERY  \n",
       "1  Mon Apr 06 22:19:49 PDT 2009    scotthamilton          0  NO_QUERY  \n",
       "2  Mon Apr 06 22:19:53 PDT 2009         mattycus          0  NO_QUERY  \n",
       "3  Mon Apr 06 22:19:57 PDT 2009          ElleCTF          0  NO_QUERY  \n",
       "4  Mon Apr 06 22:19:57 PDT 2009           Karoli          0  NO_QUERY  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to pandas DataFrame for easier manipulation\n",
    "train_df = train_data.to_pandas()\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f25cc58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600000,) (1600000,)\n",
      "@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\n",
      "0\n",
      "[0 4]\n"
     ]
    }
   ],
   "source": [
    "# Distinguish predictors and target variable\n",
    "X_train = train_df['text']\n",
    "y_train = train_df['sentiment']\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_train[0])\n",
    "print(y_train[0])\n",
    "print(y_train.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10912ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Expand predictors into vectorized features\n",
    "\n",
    "We are choosing to use TF-IDF vectorization for the text data.\n",
    "With TF-IDF, we want to capture word importance across documents, \n",
    "    rather than simplly using word counts (using CountVectorizer).\n",
    "'''\n",
    "\n",
    "# Initialize the vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=10000)\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2e0654a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00' '000' '00am' '00pm' '01' '02' '03' '04' '05' '06' '07' '08' '09'\n",
      " '10' '100' '1000' '100th' '101' '102' '103']\n",
      "(1600000, 10000)\n"
     ]
    }
   ],
   "source": [
    "# Get the feature names (i.e., words)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(feature_names[:20])\n",
    "print(X_train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20eff963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "awww: 0.3068072523807425\n",
      "bummer: 0.36625349232604393\n",
      "carr: 0.5021652350435604\n",
      "com: 0.20454775102574493\n",
      "david: 0.34649559250309886\n",
      "day: 0.18293282029404262\n",
      "got: 0.19790956165638682\n",
      "http: 0.19030721798770697\n",
      "shoulda: 0.4335162707531134\n",
      "twitpic: 0.2467245069379507\n"
     ]
    }
   ],
   "source": [
    "### Inspect Vectorized Content ####\n",
    "\n",
    "# Convert first tweet to dense and get non-zero values\n",
    "vector_0 = X_train_tfidf[0].toarray()[0]\n",
    "\n",
    "# Get non-zero features and their scores\n",
    "for idx, score in enumerate(vector_0):\n",
    "    if score > 0:\n",
    "        print(f\"{feature_names[idx]}: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf2af42",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34431583",
   "metadata": {},
   "source": [
    "Check for imbalance and visualize data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3ed1d959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "0    800000\n",
       "4    800000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check class distribution\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3668f3f0",
   "metadata": {},
   "source": [
    "## Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d749f51",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c7d8ddd",
   "metadata": {},
   "source": [
    "## Twitter Live Feed Integration (Twitter API and Machine Learning Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67645f88",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twitter-sentiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
